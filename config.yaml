# ==============================================================================
# ContentCreator - AI Video & Reel Generation Engine
# ==============================================================================
# Configuration file for the entire pipeline.
# All models run locally on GPU. No paid APIs required.
# ==============================================================================

# --- LLM (Script Parsing & Scene Planning) ---
llm:
  provider: "ollama"
  model: "mistral:latest"              # 7.2B params, fits in RAM easily
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 4096

# --- Text-to-Speech ---
tts:
  engine: "edge"                      # "edge" (Microsoft Edge TTS, free, high quality)
  language: "en"
  voice: "en-US-GuyNeural"            # See available voices: python -c "import asyncio; from src.tts_engine import TTSEngine; print(asyncio.run(TTSEngine.list_voices()))"
  rate: "+0%"                         # Speed: "-50%" to "+100%"
  volume: "+0%"                       # Volume adjustment
  pitch: "+0Hz"                       # Pitch adjustment

# --- Image Generation ---
image:
  engine: "sdxl"                      # "sdxl" or "sd15"
  model: "stabilityai/stable-diffusion-xl-base-1.0"
  refiner: "stabilityai/stable-diffusion-xl-refiner-1.0"
  use_refiner: false                  # Enable for higher quality (uses more VRAM)
  width: 1024
  height: 1024
  num_inference_steps: 30
  guidance_scale: 7.5
  negative_prompt: "blurry, low quality, distorted, deformed, ugly, bad anatomy"

# --- Video Generation ---
video:
  engine: "image_motion"              # "image_motion" (smooth Ken Burns, DEFAULT) | "animatediff" | "svd"
  # --- Advanced Ken Burns (primary — cinematic zoom/pan/drift on AI images) ---
  image_motion:
    zoom_range: [1.0, 1.20]           # max zoom factor (1.0 = no zoom, 1.2 = 20% zoom)
    pan_range: [-0.08, 0.08]          # horizontal/vertical pan as fraction of image
    duration_per_scene: 5             # fallback duration when audio not available
    fps: 24                           # output frame rate
  # --- AnimateDiff (alternative — text-to-animated-video, ~8-10 GB VRAM) ---
  # NOTE: AnimateDiff generates a fixed number of frames (typically 16 = 0.67s
  #       at 24fps). The assembler will slow-motion or loop to fill the
  #       narration. Use image_motion for best results.
  animatediff:
    base_model: "SG161222/Realistic_Vision_V5.1_noVAE"   # SD 1.5 checkpoint
    motion_adapter: "guoyww/animatediff-motion-adapter-v1-5-2"
    num_frames: 16                    # frames per clip (16-32; higher = more VRAM)
    fps: 24                           # output frame rate
    guidance_scale: 7.5
    num_inference_steps: 25
    width: 512                        # generation width  (keep ≤768 for 16 GB VRAM)
    height: 768                       # generation height (portrait for reels)
  # --- SVD (alternative — image-to-video, ~12 GB VRAM) ---
  model: "stabilityai/stable-video-diffusion-img2vid-xt"
  num_frames: 25                      # ~3 seconds at 8fps
  fps: 8
  motion_bucket_id: 127              # Higher = more motion
  noise_aug_strength: 0.02
  decode_chunk_size: 8
  # --- Ken Burns fallback (legacy settings — see image_motion above) ---
  # image_motion settings are defined above as the primary engine

# --- Music Generation ---
music:
  engine: "musicgen"                  # "musicgen" or "none"
  model: "facebook/musicgen-small"    # small=300M, medium=1.5B, large=3.3B
  duration: 30                        # seconds of music to generate
  volume: 0.3                         # relative to voiceover (0.0 - 1.0)

# --- Subtitles ---
subtitles:
  enabled: true
  engine: "whisper"                   # "whisper" for word-level timestamps
  model_size: "base"                  # tiny, base, small, medium, large
  font: "Arial-Bold"
  font_size: 48
  color: "white"
  stroke_color: "black"
  stroke_width: 2
  position: "bottom"                  # "bottom", "center", "top"
  max_words_per_line: 5

# --- Character Designer ---
characters:
  default_style: "stick_figure"       # stick_figure, cartoon, manga, doodle, whiteboard
  controlnet_model: "diffusers/controlnet-canny-sdxl-1.0"
  # Edge detection settings (for photo → sketch)
  canny_low_threshold: 50
  canny_high_threshold: 150
  # Generation settings
  num_inference_steps: 30
  guidance_scale: 7.5
  controlnet_conditioning_scale: 0.8

# --- Output Settings ---
output:
  directory: "output"
  temp_directory: "temp"
  format: "mp4"
  codec: "libx264"
  audio_codec: "aac"
  # Presets for different platforms
  presets:
    youtube:
      width: 1920
      height: 1080
      fps: 24
      bitrate: "8M"
    reels:
      width: 1080
      height: 1920
      fps: 24
      bitrate: "8M"
    shorts:
      width: 1080
      height: 1920
      fps: 24
      bitrate: "8M"

# --- Pipeline ---
pipeline:
  # Which stages to run (useful for debugging / partial runs)
  stages:
    - script_parse
    - character_design
    - tts
    - image_gen
    - video_gen
    - music_gen
    - subtitles
    - assemble
  # VRAM management: unload each model after use
  auto_unload_models: true
  # Device
  device: "cuda"                      # "cuda" or "cpu"
  # Half precision (saves VRAM)
  half_precision: true
